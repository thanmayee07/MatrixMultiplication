{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0oEDq6lh34U",
        "outputId": "ba54d2dd-0736-4fa7-9363-a83c9075729e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-ba7_3ri9\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-ba7_3ri9\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 326b0a57a80c6d0b4bad25ca7adf8138419ef1cb\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10741 sha256=1c43b610d84440f376c57bb0b3d20f87e2433fdb6f06eb98d5fd7c81c461a971\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-o_nx2vus/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDp3ccyfh7ov",
        "outputId": "2663a543-bd32-49f3-a44e-d4c14388eaf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: nvcc4jupyter\n",
            "Version: 1.2.1\n",
            "Summary: Jupyter notebook plugin to run CUDA C/C++ code\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: Andrei Nechaev <lyfaradey@yahoo.com>, Cosmin Stefan Ciocan <ciocan.cosmin98@gmail.com>\n",
            "License: MIT License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQQV8Sbxh_9y",
        "outputId": "724b9e83-fe09-4358-e310-e8e4dd140483"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmppz928fo9\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <random>\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Structure for CSR matrix\n",
        "typedef struct {\n",
        "    int rows_count;\n",
        "    int cols_count;\n",
        "    int non_zero_count;\n",
        "    float* values;\n",
        "    int* col_indices;\n",
        "    int* row_ptr;\n",
        "} csr_matrix;\n",
        "\n",
        "// Function to generate a random CSR matrix\n",
        "csr_matrix generate_random_csr_matrix(int rows, int cols, float density) {\n",
        "    csr_matrix matrix;\n",
        "    matrix.rows_count = rows;\n",
        "    matrix.cols_count = cols;\n",
        "    matrix.row_ptr = (int*)malloc((rows + 1) * sizeof(int));\n",
        "\n",
        "    // Generate random values for the matrix\n",
        "    std::random_device rd;\n",
        "    std::mt19937 gen(rd());\n",
        "    std::uniform_real_distribution<> dis(1.0, 1000.0); // Random value between 1 and 1000\n",
        "    std::uniform_int_distribution<> col_dis(0, cols - 1); // Random column index between 0 and cols - 1\n",
        "\n",
        "    int max_non_zeros = (int)(density * rows * cols);\n",
        "    matrix.non_zero_count = max_non_zeros;\n",
        "    matrix.values = (float*)malloc(max_non_zeros * sizeof(float));\n",
        "    matrix.col_indices = (int*)malloc(max_non_zeros * sizeof(int));\n",
        "\n",
        "    int current_index = 0;\n",
        "    matrix.row_ptr[0] = 0;\n",
        "    for (int i = 0; i < rows; ++i) {\n",
        "        int non_zeros_in_row = (int)(density * cols); // Determine number of non-zeros in this row\n",
        "        matrix.row_ptr[i + 1] = matrix.row_ptr[i] + non_zeros_in_row; // Update row_ptr\n",
        "        for (int j = 0; j < non_zeros_in_row; ++j) {\n",
        "            matrix.values[current_index] = dis(gen); // Random value\n",
        "            matrix.col_indices[current_index] = col_dis(gen); // Random column index\n",
        "            ++current_index;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return matrix;\n",
        "}\n",
        "\n",
        "// CUDA kernel for SpMV computation with memory coalescing and shared memory optimization\n",
        "__global__ void csr_spmv_kernel_shared(int rows_count, int* row_ptr, int* col_indices, float* values, float* x, float* y) {\n",
        "    // Define shared memory for values and column indices\n",
        "    __shared__ float shared_values[BLOCK_SIZE];\n",
        "    __shared__ int shared_col_indices[BLOCK_SIZE];\n",
        "\n",
        "    // Calculate the thread index within the block\n",
        "    int tid = threadIdx.x;\n",
        "    int row = blockIdx.x * blockDim.x + tid;\n",
        "\n",
        "    // Initialize the result for this thread's row\n",
        "    float dot = 0.0f;\n",
        "\n",
        "    // Loop over the tiles of the matrix\n",
        "    for (int tile_index = 0; tile_index < (rows_count + BLOCK_SIZE - 1) / BLOCK_SIZE; ++tile_index) {\n",
        "        // Load values and column indices for this tile into shared memory\n",
        "        int col_index = tile_index * BLOCK_SIZE + tid;\n",
        "        if (col_index < rows_count) {\n",
        "            int offset = row_ptr[col_index];\n",
        "            shared_values[tid] = values[offset + tid];\n",
        "            shared_col_indices[tid] = col_indices[offset + tid];\n",
        "        } else {\n",
        "            shared_values[tid] = 0.0f;\n",
        "            shared_col_indices[tid] = -1; // Invalid index\n",
        "        }\n",
        "\n",
        "        // Synchronize to make sure all threads have loaded the tile\n",
        "        __syncthreads();\n",
        "\n",
        "        // Perform the dot product within the tile\n",
        "        for (int i = 0; i < BLOCK_SIZE; ++i) {\n",
        "            if (shared_col_indices[i] != -1) { // Check for valid column index\n",
        "                dot += shared_values[i] * x[shared_col_indices[i]];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Synchronize before loading the next tile\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Write the result back to global memory\n",
        "    if (row < rows_count) {\n",
        "        y[row] = dot;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to perform SpMV on GPU and measure performance\n",
        "void gpu_csr_spmv_perf(const csr_matrix* matrix, const float* x, float* y) {\n",
        "    // Allocate memory on GPU\n",
        "    float *d_values, *d_x, *d_y;\n",
        "    int *d_col_indices, *d_row_ptr;\n",
        "    cudaMalloc((void**)&d_values, matrix->non_zero_count * sizeof(float));\n",
        "    cudaMalloc((void**)&d_col_indices, matrix->non_zero_count * sizeof(int));\n",
        "    cudaMalloc((void**)&d_row_ptr, (matrix->rows_count + 1) * sizeof(int));\n",
        "    cudaMalloc((void**)&d_x, matrix->cols_count * sizeof(float));\n",
        "    cudaMalloc((void**)&d_y, matrix->rows_count * sizeof(float));\n",
        "\n",
        "    // Check for memory allocation errors\n",
        "    if (!d_values || !d_col_indices || !d_row_ptr || !d_x || !d_y) {\n",
        "        fprintf(stderr, \"CUDA memory allocation error\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Copy data to GPU\n",
        "    cudaMemcpy(d_values, matrix->values, matrix->non_zero_count * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_col_indices, matrix->col_indices, matrix->non_zero_count * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_row_ptr, matrix->row_ptr, (matrix->rows_count + 1) * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_x, x, matrix->cols_count * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    int threads_per_block = BLOCK_SIZE;\n",
        "    int blocks_per_grid = (matrix->rows_count + threads_per_block - 1) / threads_per_block;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "    cudaEventRecord(start);\n",
        "\n",
        "    csr_spmv_kernel_shared<<<blocks_per_grid, threads_per_block>>>(matrix->rows_count, d_row_ptr, d_col_indices, d_values, d_x, d_y);\n",
        "\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    float milliseconds = 0;\n",
        "    cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "    // Copy result back to CPU\n",
        "    cudaMemcpy(y, d_y, matrix->rows_count * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Check for kernel launch errors\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        fprintf(stderr, \"CUDA kernel launch error: %s\\n\", cudaGetErrorString(err));\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(d_values);\n",
        "    cudaFree(d_col_indices);\n",
        "    cudaFree(d_row_ptr);\n",
        "    cudaFree(d_x);\n",
        "    cudaFree(d_y);\n",
        "\n",
        "    // Calculate total number of floating-point operations\n",
        "    double total_operations = (double)matrix->non_zero_count * 2; // Assuming one multiplication and one addition per non-zero element\n",
        "\n",
        "    // Calculate GFLOPS\n",
        "    double seconds = milliseconds / 1000.0; // Convert milliseconds to seconds\n",
        "    double gflops = (total_operations / seconds) / 1e9; // Divide by elapsed time in seconds and 1 billion\n",
        "    double throughput = total_operations / seconds; // Throughput in operations per second\n",
        "\n",
        "    // Output performance metrics\n",
        "    printf(\"Elapsed Time: %f milliseconds\\n\", milliseconds);\n",
        "    printf(\"GFLOPS: %f\\n\", gflops);\n",
        "    printf(\"Throughput: %f OPS\\n\", throughput);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define matrix dimensions and density\n",
        "    int size = 1 << 15; // 2^15\n",
        "    float density = 0.1; // Density of non-zero elements in the matrix (10%)\n",
        "\n",
        "    // Generate a random CSR matrix\n",
        "    csr_matrix matrix = generate_random_csr_matrix(size, size, density);\n",
        "\n",
        "    // Allocate memory for input and output vectors\n",
        "    float* x = (float*)malloc(size * sizeof(float));\n",
        "    float* y = (float*)malloc(size * sizeof(float));\n",
        "\n",
        "    // Check for memory allocation errors\n",
        "    if (!x || !y) {\n",
        "        fprintf(stderr, \"Memory allocation error\\n\");\n",
        "        exit(EXIT_FAILURE);\n",
        "    }\n",
        "\n",
        "    // Initialize input vector x (for demonstration, fill it with 1.0)\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        x[i] = 1.0f;\n",
        "    }\n",
        "\n",
        "    // Perform SpMV on GPU and measure performance\n",
        "    printf(\"Performance metrics for random CSR matrix:\\n\");\n",
        "    gpu_csr_spmv_perf(&matrix, x, y);\n",
        "\n",
        "    // Free memory\n",
        "    free(matrix.values);\n",
        "    free(matrix.col_indices);\n",
        "    free(matrix.row_ptr);\n",
        "    free(x);\n",
        "    free(y);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8rehX9t_8of",
        "outputId": "11f5c420-826c-4a94-963a-f6ed40a4a1c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance metrics for random CSR matrix:\n",
            "Elapsed Time: 10.840416 milliseconds\n",
            "GFLOPS: 19.809975\n",
            "Throughput: 19809974903.137856 OPS\n",
            "\n"
          ]
        }
      ]
    }
  ]
}